{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xqtPUafmApG5"
   },
   "source": [
    "# Network Friendly Recommendations Project: 1st assignment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fG_0WV4gAmv6"
   },
   "source": [
    "As mentioned in class, this assignment is basically \"part 1\" of your final project assignment. The idea is to implement a simple(r) version of the problem that can be solved with \"exact\" methods (i.e., no approximations based on neural networks or other methods) that we learned in the first lectures about MDP and Q-learning methods. I provide below my suggestions for implementing this first version of your project:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gp5UAOWzUghI"
   },
   "source": [
    "##Libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "8QwChHYuUfkD"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sys, os, time, copy, math, random\n",
    "import matplotlib.cm as cm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RKFEcvdSUZeU"
   },
   "source": [
    "##Functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "ekvJToSzBRxU"
   },
   "outputs": [],
   "source": [
    "# Print u matrix with colors based on relations\n",
    "def print_matrix(matrix,u_min):\n",
    "    RED = '\\033[91m'  # ANSI escape code for red text\n",
    "    YELLOW = '\\033[93m'  # ANSI escape code for yellow text\n",
    "    RESET = '\\033[0m'  # ANSI escape code to reset the text color\n",
    "\n",
    "    for i, row in enumerate(matrix):\n",
    "        for j, element in enumerate(row):\n",
    "            if i == j:\n",
    "                print(f\"{YELLOW}{element:.3f}{RESET}\", end=\" \")  # Print diagonal element in yellow\n",
    "            elif element < u_min:\n",
    "                print(f\"{RED}{element:.3f}{RESET}\", end=\" \")  # Print in red if smaller than min_value\n",
    "            else:\n",
    "                print(f\"{element:.3f}\", end=\" \")\n",
    "        print()\n",
    "\n",
    "# Create a symmetric matrix\n",
    "def create_symmetric_matrix(K):\n",
    "    matrix = [[random.random() if i != j else 0 for j in range(K)] for i in range(K)]\n",
    "\n",
    "    # Make the matrix symmetric by copying the upper triangle to the lower triangle\n",
    "    for i in range(K):\n",
    "        for j in range(i+1, K):\n",
    "            matrix[j][i] = matrix[i][j]\n",
    "\n",
    "    return matrix\n",
    "\n",
    "# Choose which C of the K items will be cached\n",
    "def random_cached_items(K, C):\n",
    "    reward = [-1] * K  # Create a vector of length K with all 0 elements\n",
    "\n",
    "    # Select C random indices\n",
    "    indices = random.sample(range(K), C)\n",
    "\n",
    "    # Set the selected indices to True\n",
    "    for index in indices:\n",
    "        reward[index] = 0\n",
    "\n",
    "    return reward\n",
    "\n",
    "# Random recommendation for current content watched\n",
    "def random_recommendation(K, N, curr_content):\n",
    "    recom = []  # Create an empty vector\n",
    "\n",
    "    for _ in range(N):\n",
    "        random_number = random.randint(0, K-1)\n",
    "        while random_number == curr_content or random_number in recom:  # Check if random number is equal to curr_content or is already in recom\n",
    "            random_number = random.randint(0, K-1)  # Generate a new random number\n",
    "        recom.append(random_number)\n",
    "    recom.sort()\n",
    "    return recom\n",
    "\n",
    "# Are all the recommended videos relevant to the current content being watched?\n",
    "def all_relevant(N, curr_content, u, recom, u_min):\n",
    "    all_relevant = True\n",
    "    for i in range(N):\n",
    "        if u[curr_content][recom[i]] < u_min: # Check if at least one is irrelevant\n",
    "            all_relevant = False                # If one is irrelevant return false\n",
    "            break\n",
    "    return all_relevant\n",
    "\n",
    "# User chooses the next video to watch\n",
    "def user_chooses(K, N, q, u, u_min, a, recom, curr_content):\n",
    "    if random.uniform(0, 1) > q:\n",
    "        if all_relevant(N, curr_content, u, recom, u_min):  # If all recommended are relevant\n",
    "            if random.uniform(0, 1) < a:  # User chooses one of the recommended\n",
    "                new_content = recom[random.randint(0, N-1)]\n",
    "            else:\n",
    "                new_content = random.choice([x for x in range(K) if x != curr_content])\n",
    "        else:                                               # If at least one recommended isn't relevant\n",
    "            new_content = random.choice([x for x in range(K) if x != curr_content])\n",
    "    else:\n",
    "        new_content = -1\n",
    "    return new_content\n",
    "\n",
    "# Environment probability of user moving to next content given that he watches current content\n",
    "# and he is being recommended the list recom\n",
    "def env_prob(K, N, u, u_min, a, recom, curr_content, next_content):\n",
    "    if curr_content==next_content:  # No possibility that the user watches the same content consequently\n",
    "        env_prob = 0.0\n",
    "    else:\n",
    "        if all_relevant(N, curr_content, u, recom, u_min):  # If all recommended are relevant\n",
    "        #print(\"All Relevant\")\n",
    "            if next_content in recom:     # If the next content was recommended\n",
    "                env_prob = a/N + (1-a)/(K-1)\n",
    "            else:                         # If the next content wasn't recommended\n",
    "                env_prob = (1-a)/(K-1)\n",
    "        else:                           # If at least one recommended isn't relevant\n",
    "            #print(\"Not Relevant\")\n",
    "            env_prob = 1/(K-1)\n",
    "    return env_prob\n",
    "\n",
    "# All possible recommendations for state s\n",
    "def possible_recom(K, N, s):\n",
    "    items = list(range(K))\n",
    "    items.remove(s)  # Remove 's' from the list of items\n",
    "\n",
    "    def generate_combinations(curr_set, remaining_items):\n",
    "        if len(curr_set) == N:\n",
    "            return [curr_set]\n",
    "\n",
    "        all_combinations = []\n",
    "        for i, item in enumerate(remaining_items):\n",
    "            new_set = curr_set + [item]\n",
    "            new_remaining = remaining_items[i+1:]\n",
    "            all_combinations.extend(generate_combinations(new_set, new_remaining))\n",
    "\n",
    "        return all_combinations\n",
    "\n",
    "    combinations = generate_combinations([], items)\n",
    "    return list(map(list, combinations))\n",
    "\n",
    "# All possible recommendations for all states\n",
    "def all_states_possible_recom(K, N):\n",
    "\n",
    "    all_combinations = []\n",
    "    for s in range(K):\n",
    "        state_combination = possible_recom(K, N, s)\n",
    "        all_combinations.append(state_combination)\n",
    "\n",
    "    return all_combinations\n",
    "\n",
    "# Run \"sessions_num\" Monte Carlo episodes and compute the mean loss and the total loss\n",
    "def monte_carlo_sessions(sessions_num, policy, reward, K, N, u_min, a, q, u):\n",
    "    print(\"> Running Monte Carlo sessions...\")\n",
    "    total_loss = 0\n",
    "    content_watched = 0\n",
    "    for _ in range(sessions_num):\n",
    "        curr_content = random.randint(0, K-1) # The first item viewed is random\n",
    "\n",
    "        while True:\n",
    "            recom = policy[curr_content]  # Recommend N items based on the policy\n",
    "            curr_content = user_chooses(K, N, q, u, u_min, a, recom, curr_content)\n",
    "            if curr_content == -1:\n",
    "                break\n",
    "            if reward[curr_content]==-1:\n",
    "                total_loss += 1\n",
    "            content_watched+=1\n",
    "\n",
    "    if content_watched == 0:\n",
    "        mean_loss = 0\n",
    "    else:\n",
    "        mean_loss = total_loss/content_watched\n",
    "\n",
    "    return mean_loss, total_loss\n",
    "\n",
    "# Find all the values above u_min\n",
    "def find_values_above_min(matrix, u_min):\n",
    "    result = []\n",
    "    for i, row in enumerate(matrix):\n",
    "        row_result = []\n",
    "        for j, value in enumerate(row):\n",
    "            if value > u_min:\n",
    "                row_result.append(j)\n",
    "        result.append(row_result)\n",
    "    return result\n",
    "\n",
    "# Disable\n",
    "def blockPrint():\n",
    "    sys.stdout = open(os.devnull, 'w')\n",
    "\n",
    "# Restore\n",
    "def enablePrint():\n",
    "    sys.stdout = sys.__stdout__\n",
    "\n",
    "def create_matrix(data):\n",
    "    K = len(data)\n",
    "    N = len(data[0])\n",
    "\n",
    "    matrix = [[None] * N for _ in range(K)]\n",
    "\n",
    "    for i in range(K):\n",
    "        matrix[i] = list(data[i])\n",
    "\n",
    "    return matrix\n",
    "\n",
    "def create_tuple_list(matrix):\n",
    "    K = len(matrix)\n",
    "    N = len(matrix[0])\n",
    "\n",
    "    tuple_list = []\n",
    "\n",
    "    for i in range(K):\n",
    "        rounded_values = [int(value) for value in matrix[i]]\n",
    "        tuple_list.append(rounded_values)\n",
    "\n",
    "    return tuple_list\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wkbhZ5fGA2Dc"
   },
   "source": [
    "### Environment (Content Catalogue):\n",
    "- $K$ content items. I would pick $K \\leq 100$ initially, as the code might get pretty slow otherwise.\n",
    "- For every pair of items, $i$ and $j$, create a random value $u_{ij}$ in $[0,1]$ that indicates how related content $j$ is to content $i$. This is basically a random array $U$ of size $K \\times K$ (you can choose to make it symmetric or not). Assume the diagonal of this array (i.e., all elements $u_{ii}$) are equal to 0 (to avoid recommending the same item just watched).\n",
    "- Assume there is a threshold $u_{\\text{min}}$ in $[0,1]$ below which two contents are \"irrelevant\" (this is an input parameter to play around with and observe its impact).\n",
    "- Assume $C$ out of these contents are cached (use values of $C \\leq 0.2K$). Cached items have cost 0, and non-cached have cost 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pA6edJp8DHtr",
    "outputId": "a419fc15-685e-4fce-dfd0-8faa8e5d84a5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[93m0.000\u001b[0m \u001b[91m0.394\u001b[0m \u001b[91m0.153\u001b[0m \u001b[91m0.128\u001b[0m \u001b[91m0.308\u001b[0m 0.799 \u001b[91m0.297\u001b[0m \u001b[91m0.457\u001b[0m \u001b[91m0.084\u001b[0m 0.582 0.977 \u001b[91m0.203\u001b[0m 0.788 \u001b[91m0.012\u001b[0m \u001b[91m0.320\u001b[0m \u001b[91m0.333\u001b[0m 0.973 0.892 \u001b[91m0.379\u001b[0m \u001b[91m0.314\u001b[0m \n",
      "\u001b[91m0.394\u001b[0m \u001b[93m0.000\u001b[0m \u001b[91m0.076\u001b[0m \u001b[91m0.185\u001b[0m \u001b[91m0.465\u001b[0m \u001b[91m0.246\u001b[0m 0.724 0.603 \u001b[91m0.127\u001b[0m 0.926 0.748 0.535 0.964 \u001b[91m0.415\u001b[0m \u001b[91m0.057\u001b[0m \u001b[91m0.097\u001b[0m 0.576 0.614 0.722 0.766 \n",
      "\u001b[91m0.153\u001b[0m \u001b[91m0.076\u001b[0m \u001b[93m0.000\u001b[0m 0.907 \u001b[91m0.122\u001b[0m 0.690 0.906 0.807 \u001b[91m0.451\u001b[0m 0.949 0.665 \u001b[91m0.312\u001b[0m 0.505 \u001b[91m0.264\u001b[0m 0.846 \u001b[91m0.268\u001b[0m \u001b[91m0.237\u001b[0m \u001b[91m0.055\u001b[0m 0.750 \u001b[91m0.297\u001b[0m \n",
      "\u001b[91m0.128\u001b[0m \u001b[91m0.185\u001b[0m 0.907 \u001b[93m0.000\u001b[0m 0.824 0.801 0.810 0.667 \u001b[91m0.159\u001b[0m 0.660 \u001b[91m0.418\u001b[0m \u001b[91m0.435\u001b[0m \u001b[91m0.297\u001b[0m \u001b[91m0.375\u001b[0m \u001b[91m0.345\u001b[0m \u001b[91m0.189\u001b[0m 0.557 0.808 0.790 0.879 \n",
      "\u001b[91m0.308\u001b[0m \u001b[91m0.465\u001b[0m \u001b[91m0.122\u001b[0m 0.824 \u001b[93m0.000\u001b[0m \u001b[91m0.159\u001b[0m \u001b[91m0.401\u001b[0m 0.605 \u001b[91m0.336\u001b[0m \u001b[91m0.259\u001b[0m \u001b[91m0.138\u001b[0m \u001b[91m0.077\u001b[0m \u001b[91m0.164\u001b[0m 0.553 0.875 \u001b[91m0.491\u001b[0m \u001b[91m0.291\u001b[0m \u001b[91m0.446\u001b[0m 0.798 \u001b[91m0.110\u001b[0m \n",
      "0.799 \u001b[91m0.246\u001b[0m 0.690 0.801 \u001b[91m0.159\u001b[0m \u001b[93m0.000\u001b[0m \u001b[91m0.357\u001b[0m \u001b[91m0.049\u001b[0m \u001b[91m0.409\u001b[0m \u001b[91m0.091\u001b[0m \u001b[91m0.400\u001b[0m \u001b[91m0.279\u001b[0m 0.998 \u001b[91m0.420\u001b[0m \u001b[91m0.363\u001b[0m 0.590 \u001b[91m0.453\u001b[0m 0.920 0.916 0.747 \n",
      "\u001b[91m0.297\u001b[0m 0.724 0.906 0.810 \u001b[91m0.401\u001b[0m \u001b[91m0.357\u001b[0m \u001b[93m0.000\u001b[0m \u001b[91m0.272\u001b[0m 0.522 \u001b[91m0.145\u001b[0m 0.916 \u001b[91m0.332\u001b[0m 0.845 0.591 \u001b[91m0.369\u001b[0m 0.987 0.621 0.749 \u001b[91m0.297\u001b[0m 0.592 \n",
      "\u001b[91m0.457\u001b[0m 0.603 0.807 0.667 0.605 \u001b[91m0.049\u001b[0m \u001b[91m0.272\u001b[0m \u001b[93m0.000\u001b[0m \u001b[91m0.295\u001b[0m 0.647 0.739 \u001b[91m0.170\u001b[0m 0.922 0.554 0.742 \u001b[91m0.088\u001b[0m 0.993 \u001b[91m0.204\u001b[0m 0.733 0.728 \n",
      "\u001b[91m0.084\u001b[0m \u001b[91m0.127\u001b[0m \u001b[91m0.451\u001b[0m \u001b[91m0.159\u001b[0m \u001b[91m0.336\u001b[0m \u001b[91m0.409\u001b[0m 0.522 \u001b[91m0.295\u001b[0m \u001b[93m0.000\u001b[0m 0.682 0.685 \u001b[91m0.309\u001b[0m \u001b[91m0.353\u001b[0m \u001b[91m0.080\u001b[0m \u001b[91m0.040\u001b[0m \u001b[91m0.257\u001b[0m \u001b[91m0.498\u001b[0m 0.662 0.539 \u001b[91m0.086\u001b[0m \n",
      "0.582 0.926 0.949 0.660 \u001b[91m0.259\u001b[0m \u001b[91m0.091\u001b[0m \u001b[91m0.145\u001b[0m 0.647 0.682 \u001b[93m0.000\u001b[0m 0.880 \u001b[91m0.287\u001b[0m \u001b[91m0.446\u001b[0m \u001b[91m0.164\u001b[0m 0.517 0.997 \u001b[91m0.049\u001b[0m 0.993 0.801 0.526 \n",
      "0.977 0.748 0.665 \u001b[91m0.418\u001b[0m \u001b[91m0.138\u001b[0m \u001b[91m0.400\u001b[0m 0.916 0.739 0.685 0.880 \u001b[93m0.000\u001b[0m 0.577 \u001b[91m0.292\u001b[0m \u001b[91m0.302\u001b[0m \u001b[91m0.182\u001b[0m \u001b[91m0.276\u001b[0m \u001b[91m0.169\u001b[0m \u001b[91m0.147\u001b[0m 0.727 \u001b[91m0.385\u001b[0m \n",
      "\u001b[91m0.203\u001b[0m 0.535 \u001b[91m0.312\u001b[0m \u001b[91m0.435\u001b[0m \u001b[91m0.077\u001b[0m \u001b[91m0.279\u001b[0m \u001b[91m0.332\u001b[0m \u001b[91m0.170\u001b[0m \u001b[91m0.309\u001b[0m \u001b[91m0.287\u001b[0m 0.577 \u001b[93m0.000\u001b[0m \u001b[91m0.374\u001b[0m 0.832 0.867 \u001b[91m0.134\u001b[0m \u001b[91m0.498\u001b[0m 0.974 \u001b[91m0.126\u001b[0m \u001b[91m0.115\u001b[0m \n",
      "0.788 0.964 0.505 \u001b[91m0.297\u001b[0m \u001b[91m0.164\u001b[0m 0.998 0.845 0.922 \u001b[91m0.353\u001b[0m \u001b[91m0.446\u001b[0m \u001b[91m0.292\u001b[0m \u001b[91m0.374\u001b[0m \u001b[93m0.000\u001b[0m \u001b[91m0.179\u001b[0m 0.639 \u001b[91m0.192\u001b[0m 0.949 \u001b[91m0.465\u001b[0m 0.502 \u001b[91m0.173\u001b[0m \n",
      "\u001b[91m0.012\u001b[0m \u001b[91m0.415\u001b[0m \u001b[91m0.264\u001b[0m \u001b[91m0.375\u001b[0m 0.553 \u001b[91m0.420\u001b[0m 0.591 0.554 \u001b[91m0.080\u001b[0m \u001b[91m0.164\u001b[0m \u001b[91m0.302\u001b[0m 0.832 \u001b[91m0.179\u001b[0m \u001b[93m0.000\u001b[0m \u001b[91m0.482\u001b[0m \u001b[91m0.322\u001b[0m 0.931 0.783 0.884 \u001b[91m0.145\u001b[0m \n",
      "\u001b[91m0.320\u001b[0m \u001b[91m0.057\u001b[0m 0.846 \u001b[91m0.345\u001b[0m 0.875 \u001b[91m0.363\u001b[0m \u001b[91m0.369\u001b[0m 0.742 \u001b[91m0.040\u001b[0m 0.517 \u001b[91m0.182\u001b[0m 0.867 0.639 \u001b[91m0.482\u001b[0m \u001b[93m0.000\u001b[0m \u001b[91m0.135\u001b[0m 0.942 0.965 \u001b[91m0.473\u001b[0m \u001b[91m0.269\u001b[0m \n",
      "\u001b[91m0.333\u001b[0m \u001b[91m0.097\u001b[0m \u001b[91m0.268\u001b[0m \u001b[91m0.189\u001b[0m \u001b[91m0.491\u001b[0m 0.590 0.987 \u001b[91m0.088\u001b[0m \u001b[91m0.257\u001b[0m 0.997 \u001b[91m0.276\u001b[0m \u001b[91m0.134\u001b[0m \u001b[91m0.192\u001b[0m \u001b[91m0.322\u001b[0m \u001b[91m0.135\u001b[0m \u001b[93m0.000\u001b[0m \u001b[91m0.154\u001b[0m 0.622 0.622 0.596 \n",
      "0.973 0.576 \u001b[91m0.237\u001b[0m 0.557 \u001b[91m0.291\u001b[0m \u001b[91m0.453\u001b[0m 0.621 0.993 \u001b[91m0.498\u001b[0m \u001b[91m0.049\u001b[0m \u001b[91m0.169\u001b[0m \u001b[91m0.498\u001b[0m 0.949 0.931 0.942 \u001b[91m0.154\u001b[0m \u001b[93m0.000\u001b[0m 0.725 0.826 \u001b[91m0.308\u001b[0m \n",
      "0.892 0.614 \u001b[91m0.055\u001b[0m 0.808 \u001b[91m0.446\u001b[0m 0.920 0.749 \u001b[91m0.204\u001b[0m 0.662 0.993 \u001b[91m0.147\u001b[0m 0.974 \u001b[91m0.465\u001b[0m 0.783 0.965 0.622 0.725 \u001b[93m0.000\u001b[0m 0.934 0.755 \n",
      "\u001b[91m0.379\u001b[0m 0.722 0.750 0.790 0.798 0.916 \u001b[91m0.297\u001b[0m 0.733 0.539 0.801 0.727 \u001b[91m0.126\u001b[0m 0.502 0.884 \u001b[91m0.473\u001b[0m 0.622 0.826 0.934 \u001b[93m0.000\u001b[0m 0.573 \n",
      "\u001b[91m0.314\u001b[0m 0.766 \u001b[91m0.297\u001b[0m 0.879 \u001b[91m0.110\u001b[0m 0.747 0.592 0.728 \u001b[91m0.086\u001b[0m 0.526 \u001b[91m0.385\u001b[0m \u001b[91m0.115\u001b[0m \u001b[91m0.173\u001b[0m \u001b[91m0.145\u001b[0m \u001b[91m0.269\u001b[0m 0.596 \u001b[91m0.308\u001b[0m 0.755 0.573 \u001b[93m0.000\u001b[0m \n",
      "[-1, -1, -1, 0, -1, -1, 0, 0, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1]\n"
     ]
    }
   ],
   "source": [
    "K = 20      # Number of content items\n",
    "u_min = 0.5 # Minimum below which two contents are \"irrelevant\"\n",
    "C = int(np.floor(0.2*K))   # Number of cached content items\n",
    "u = create_symmetric_matrix(K)        # Create the matrix of relativity\n",
    "reward = random_cached_items(K, C) # Create a vector that checks if the given item is cached\n",
    "print_matrix(u,u_min)\n",
    "print(reward)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JjrY85SrDVyH"
   },
   "source": [
    "###Environment (User Model):\n",
    "\n",
    "- A user might watch multiple items, one after the other during a session.\n",
    "- After a user watches a content, $N = 2$ new items are recommended.\n",
    "- With probability $q$ (input parameter): she ends the viewing session (i.e., does not watch another video).\n",
    "- With probability $1-q$, she proceeds with one more video as follows:\n",
    "  - If ALL $N$ recommended are \"relevant\" (i.e., have higher $u_{ij}$ than $u_{\\text{min}}$), then\n",
    "    - With probability $\\alpha$ (input parameter): the user picks one of the $N$ recommended items (with equal probability).\n",
    "    - With probability $1-\\alpha$: the user picks any item $k$ from the entire catalogue of ${K}$ items with probability $p_k$ (you can assume for simplicity that $p_k = {\\frac{1}{K}}$, i.e., uniform).\n",
    "  - If at least one of the $N$ recommendations is irrelevant, then again the user picks any item $k$ from the entire catalogue of $K$ items with probability $p_k$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jo_0hidUEJjk",
    "outputId": "025b9b2c-7058-4b35-9bf0-0fc167f9ad03"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 6, 10, 5, 15, 9, 18, 16, 7, 14, 5, 6, 9, 4, 19, 8, 14, 10, 17, 15, 13, 8, 18, 14, 16, 6]\n"
     ]
    }
   ],
   "source": [
    "N = 2         # Number of recommended content items\n",
    "q = 0.05      # Probability of a session ending\n",
    "a = 1         # Probability of choosing a recommended content item\n",
    "round = 0     # Number of content item viewed during this session\n",
    "history = []  # The history of content items viewed during this session\n",
    "curr_content = random.randint(0, K-1) # The first item viewed is random\n",
    "history.append(curr_content)  # Append first item in history\n",
    "\n",
    "while True:\n",
    "    #print(\"Round: \"+str(round))\n",
    "    #print(\"Current content: \"+str(curr_content))\n",
    "    #### THIS WILL BE REPLACED BY OUR ALGORITHMS ####\n",
    "    recom = random_recommendation(K, N, curr_content)  # Recommend N random items\n",
    "    #################################################\n",
    "    #print(\"Recommendation: \"+str(recom))\n",
    "    curr_content = user_chooses(K, N, q, u, u_min, a, recom, curr_content)\n",
    "    if curr_content == -1:\n",
    "        break\n",
    "    history.append(curr_content)\n",
    "    round+=1\n",
    "\n",
    "print(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yUoi2dCg_Gg3"
   },
   "source": [
    "### Control variables and objective:\n",
    "- Your algorithm must learn/optimize: for every possible item $i$ the user\n",
    "might watch, a tuple of $N$ items to recommend.\n",
    "- Objective: minimize the average total cost of items watched during a\n",
    "session."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zuD_VD9E_Qh_"
   },
   "source": [
    "### Algorithms to implement:\n",
    "1. Policy Iteration to optimize recommendations, assuming all environment\n",
    "parameters are known."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3ds5mqqW5Wp1"
   },
   "source": [
    "### Policy evaluation\n",
    "In general:\n",
    "\n",
    "$V_{i+1}(s)=\\color{yellow}{\\sum_{a\\in\\mathcal{A}}\\pi(a|s)}\\left(\\mathcal{R}_s^a+\\gamma \\sum_{s'\\in\\mathcal{S}}\\mathcal{P}_{ss'}^{a}V_i(s')\\right)$\n",
    "\n",
    "Where:\n",
    "  - $\\color{yellow}{\\text{One action per state in every policy so it isn't needed}}$\n",
    "  - $\\pi(a|s)$ is the set of actions that are allowed in this policy given state s\n",
    "  - $\\mathcal{R}_s^a$ is the reward of state s\n",
    "  - $\\gamma$ is equal to $1-q$\n",
    "  - $\\mathcal{P}_{ss'}^{a}$ is the environment probability of moving to state $s'$ given we are at state $s$ and we make action $a$\n",
    "  - $V_i(s')$ is the last iteration's value function value of the next state\n",
    "\n",
    "We could write it like this:\n",
    "\n",
    " $V_{i+1}(s)=\\pi(a|s)\\left(\\mathcal{R}_s+(1-q) \\sum_{s'\\in\\mathcal{S}}\\mathcal{P}_{ss'}^aV_i(s')\\right)$\n",
    "\n",
    "The values of $\\mathcal{P}_{ss'}^{a}$ are computed like this:\n",
    "\n",
    "### Policy Improvement\n",
    "\n",
    "$q_{\\pi}(s,a) = \\mathcal{R}_{s}+\\sum_{s'\\in\\mathcal{S}}\\mathcal{P}_{ss'}^aV_{\\pi}(s')$\n",
    "\n",
    "$\\pi'(s)=argmax_{a\\in \\mathcal{A}}(q_{\\pi}(s,a))$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "cellView": "code",
    "id": "FiQkZX0MDRuk"
   },
   "outputs": [],
   "source": [
    "def policy_iteration(K, N, u, u_min, a, q, reward, initial_policy):\n",
    "    policy = []\n",
    "    if initial_policy == 0:\n",
    "        policy = [random_recommendation(K, N, i) for i in range(K)]\n",
    "    else:\n",
    "        policy = copy.deepcopy(initial_policy)                      # Initial random policy\n",
    "    V = np.zeros(K)                                                 # Initial value function\n",
    "    Q = np.zeros((K,int(math.comb(K-1,N))) # Initial Q function\n",
    "    sessions_num = 100000   # Number of episodes run on Monte Carlo\n",
    "    mean_loss = []\n",
    "    total_loss = []\n",
    "\n",
    "    def policy_evaluation(policy):\n",
    "        print(\"> Evaluating policy...\")\n",
    "        threshold = 1e-10\n",
    "        i=0\n",
    "        while True:\n",
    "            max_diff = 0\n",
    "            for s in range(K):\n",
    "                old_V = V[s]\n",
    "                V[s] = reward[s] + (1-q) * sum(env_prob(K, N, u, u_min, a, policy[s], s, s_next) * V[s_next] for s_next in range(K))  # Calculate the value function for the current policy\n",
    "                max_diff = max(max_diff, np.abs(old_V-V[s]))\n",
    "                i+=1\n",
    "            if max_diff < threshold:    # If the max_diff is lower than the threshold convergence is reached\n",
    "                break\n",
    "\n",
    "    def policy_improvement(policy):\n",
    "        print(\"> Improving policy...\")\n",
    "        for s in range(K):   # For all states\n",
    "            i=0\n",
    "            all_recom = possible_recom(K, N, s)\n",
    "            for recom in all_recom:                 # For all possible recommendations of this state\n",
    "                Q[s][i] = reward[s] + (1-q) * sum(env_prob(K, N, u, u_min, a, recom, s, s_next) * V[s_next] for s_next in range(K))\n",
    "                i+=1\n",
    "            policy[s] = all_recom[np.argmax(Q[s])]  # Calculate the new policy by picking the best action for each state\n",
    "\n",
    "    iteration = 0\n",
    "    monte_carlo_elapsed_time = 0\n",
    "    start_time = time.time()\n",
    "    while True:\n",
    "        last_policy = list(policy)\n",
    "        #print(\"\\n--------------------------------------------\")\n",
    "        #print(\"----------------- Policy \"+str(iteration)+\" -----------------\")\n",
    "        #print(\"--------------------------------------------\\n\")\n",
    "        #print(policy)\n",
    "        monte_carlo_start_time = time.time()\n",
    "        mean_loss_i = 0\n",
    "        total_loss_i = 0\n",
    "        #mean_loss_i, total_loss_i = monte_carlo_sessions(sessions_num, policy, reward, K, N, u_min, a, q, u)\n",
    "        monte_carlo_end_time = time.time()\n",
    "        monte_carlo_elapsed_time += monte_carlo_end_time - monte_carlo_start_time\n",
    "        mean_loss.append(mean_loss_i)\n",
    "        total_loss.append(total_loss_i)\n",
    "        #print(\"Mean Loss \"+str(iteration)+\":\")\n",
    "        #print(mean_loss[-1])\n",
    "        policy_evaluation(policy)\n",
    "        #print(\"Value Function \"+str(iteration)+\":\")\n",
    "        #print(V)\n",
    "        policy_improvement(policy)\n",
    "        iteration+=1\n",
    "        if last_policy == policy or iteration == 30:                # If two consequent policies are the same break\n",
    "            print(\"\\033[91mSAME POLICY!\\033[0m\")\n",
    "            break\n",
    "    end_time = time.time()\n",
    "    elapsed_time = end_time - start_time - monte_carlo_elapsed_time\n",
    "    print(\"Iterations needed: \"+ str(iteration))\n",
    "    print(\"Optimal Policy: \")\n",
    "    print(policy)\n",
    "    policy_evaluation(policy)\n",
    "    print(\"Optimal Value Function \"+str(iteration))\n",
    "    print(V)\n",
    "    print(\"Mean Losses for each iteration: \")\n",
    "    print(mean_loss)\n",
    "\n",
    "    return policy, mean_loss, iteration, elapsed_time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d9xVnI6K7aIk"
   },
   "source": [
    "## Running a toy example\n",
    "$K$ = 10, $N$ = 2, $u_{min}$ = 0.5, $a$ = 1, $q$=0.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uk2cuTV9kfHR",
    "outputId": "146921c3-129e-49fb-de82-4b09b5c7ea1b",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State: 0 -> [1, 4, 6, 7, 8]\n",
      "State: 1 -> [0, 2, 3, 5, 6]\n",
      "State: 2 -> [1, 4, 6]\n",
      "State: 3 -> [1, 5, 8]\n",
      "State: 4 -> [0, 2, 5, 8]\n",
      "State: 5 -> [1, 3, 4, 7]\n",
      "State: 6 -> [0, 1, 2, 9]\n",
      "State: 7 -> [0, 5, 9]\n",
      "State: 8 -> [0, 3, 4, 9]\n",
      "State: 9 -> [6, 7, 8]\n",
      "> Evaluating policy...\n",
      "> Improving policy...\n",
      "> Evaluating policy...\n",
      "> Improving policy...\n",
      "> Evaluating policy...\n",
      "> Improving policy...\n",
      "> Evaluating policy...\n",
      "> Improving policy...\n",
      "> Evaluating policy...\n",
      "> Improving policy...\n",
      "\u001b[91mSAME POLICY!\u001b[0m\n",
      "Iterations needed: 5\n",
      "Optimal Policy: \n",
      "[[1, 6], [0, 6], [1, 6], [1, 5], [0, 2], [1, 4], [0, 1], [0, 5], [0, 4], [0, 1]]\n",
      "> Evaluating policy...\n",
      "Optimal Value Function 5\n",
      "[-6.44067797 -6.44067797 -7.44067797 -7.70081594 -7.59364407 -7.66630297\n",
      " -7.11864407 -7.70081594 -7.66630297 -7.94223687]\n",
      "Mean Losses for each iteration: \n",
      "[0, 0, 0, 0, 0]\n",
      "0.5273065567016602\n"
     ]
    }
   ],
   "source": [
    "toy_K = 10\n",
    "toy_N = 2\n",
    "toy_u = [[0.0, 0.9, 0.1, 0.1, 0.9, 0.1, 0.9, 0.9, 0.9, 0.1],\n",
    "         [0.9, 0.0, 0.9, 0.9, 0.1, 0.9, 0.9, 0.1, 0.1, 0.1],\n",
    "         [0.1, 0.9, 0.0, 0.1, 0.9, 0.1, 0.9, 0.1, 0.1, 0.1],\n",
    "         [0.1, 0.9, 0.1, 0.0, 0.1, 0.9, 0.1, 0.1, 0.9, 0.1],\n",
    "         [0.9, 0.1, 0.9, 0.1, 0.0, 0.9, 0.1, 0.1, 0.9, 0.1],\n",
    "         [0.1, 0.9, 0.1, 0.9, 0.9, 0.0, 0.1, 0.9, 0.1, 0.1],\n",
    "         [0.9, 0.9, 0.9, 0.1, 0.1, 0.1, 0.0, 0.1, 0.1, 0.9],\n",
    "         [0.9, 0.1, 0.1, 0.1, 0.1, 0.9, 0.1, 0.0, 0.1, 0.9],\n",
    "         [0.9, 0.1, 0.1, 0.9, 0.9, 0.1, 0.1, 0.1, 0.0, 0.9],\n",
    "         [0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.9, 0.9, 0.9, 0.0]]\n",
    "relations = find_values_above_min(toy_u, u_min)\n",
    "for i in range(toy_K):\n",
    "    print(\"State: \"+str(i)+\" -> \"+str(relations[i]))\n",
    "toy_u_min = 0.5\n",
    "toy_a = 1\n",
    "toy_q = 0.05\n",
    "toy_reward = [0, 0, -1, -1, -1, -1, -1, -1, -1, -1]\n",
    "opt_toy_policy, mean_loss_toy, iteration_toy, elapsed_time_toy = policy_iteration(toy_K, toy_N, toy_u, toy_u_min, toy_a, toy_q, toy_reward, 0)\n",
    "print(elapsed_time_toy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jb_01B5C5yUm"
   },
   "source": [
    "## Experimenting with $a$ values\n",
    "$a$ = [0.25, 0.5, 0.75, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nCFT-HyWkfHO"
   },
   "outputs": [],
   "source": [
    "mean_loss_list = []\n",
    "iterations_list = []\n",
    "elapsed_time_list = []\n",
    "a_list = [0.25, 0.5, 0.75, 1]\n",
    "initial_policy = [random_recommendation(K, N, i) for i in range(K)]\n",
    "for a in a_list:\n",
    "    opt_policy, mean_loss, iteration, elapsed_time = policy_iteration(K, N, u, u_min, a, q, reward, initial_policy)\n",
    "    mean_loss_list.append(mean_loss)\n",
    "    iterations_list.append(iteration)\n",
    "    elapsed_time_list.append(elapsed_time)\n",
    "print(mean_loss_list)\n",
    "print(iterations_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zoRQtbVGkfHO"
   },
   "outputs": [],
   "source": [
    "cmap = cm.get_cmap('summer')\n",
    "plt.title(\"Policy Iteration Performance based on a\")\n",
    "plt.ylim(0, 1)\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Mean Loss\")\n",
    "for i in range(len(a_list)):\n",
    "    plt.plot(list(range(iterations_list[i])),\n",
    "             mean_loss_list[i], color=cmap(i/len(a_list)),\n",
    "             label='a='+str(a_list[i]))\n",
    "    plt.text(iterations_list[i]-1.5,mean_loss_list[i][1]+0.01,str(int(elapsed_time_list[i]*1000))+\" ms\",color=cmap(i/len(a_list)))\n",
    "plt.legend()\n",
    "plt.show\n",
    "#plt.savefig(\"parameter_a_iteration.png\", dpi=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AfQ69La26SUE"
   },
   "source": [
    "## Experimenting with $N$ values\n",
    "$N$ = [2, 3, 4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FyLywBwdkfHO",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mean_loss_list = []\n",
    "iterations_list = []\n",
    "elapsed_time_list = []\n",
    "a = 0.8\n",
    "N_list = [2, 3, 4]\n",
    "for N in N_list:\n",
    "    opt_policy, mean_loss, iteration, elapsed_time = policy_iteration(K, N, u, u_min, a, q, reward, 0)\n",
    "    mean_loss_list.append(mean_loss)\n",
    "    iterations_list.append(iteration)\n",
    "    elapsed_time_list.append(elapsed_time)\n",
    "print(mean_loss_list)\n",
    "print(iterations_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-pRSaQb7kfHO"
   },
   "outputs": [],
   "source": [
    "cmap = cm.get_cmap('summer')\n",
    "plt.title(\"Policy Iteration Performance based on Ν\")\n",
    "plt.ylim(0, 1)\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Mean Loss\")\n",
    "for i in range(len(N_list)):\n",
    "    plt.plot(list(range(iterations_list[i])),\n",
    "             mean_loss_list[i], color=cmap(i/len(N_list)),\n",
    "             label='N='+str(N_list[i]))\n",
    "    plt.text(iterations_list[i]-1.5,mean_loss_list[i][1]+0.01,str(int(elapsed_time_list[i]*1000))+\" ms\",color=cmap(i/len(N_list)))\n",
    "plt.legend()\n",
    "plt.show\n",
    "#plt.savefig(\"parameter_N_iteration.png\", dpi=300)\n",
    "print(elapsed_time_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cngh98rl6dyg"
   },
   "source": [
    "## Experimenting with $K$ values\n",
    "$K$ = [20, 40, 60, 80, 100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DdGZt6NWkfHP",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mean_loss_list = []\n",
    "iterations_list = []\n",
    "elapsed_time_list = []\n",
    "N = 2\n",
    "K_list = [20, 40, 60, 80, 100]\n",
    "u_local = create_symmetric_matrix(K_list[-1])\n",
    "for K_local in K_list:\n",
    "    C_local = int(np.floor(0.2*K_local))\n",
    "    reward_local = random_cached_items(K_local, C_local)\n",
    "    opt_policy, mean_loss, iteration, elapsed_time = policy_iteration(K_local, N, u_local, u_min, a, q, reward_local, 0)\n",
    "    mean_loss_list.append(mean_loss)\n",
    "    iterations_list.append(iteration)\n",
    "    elapsed_time_list.append(elapsed_time)\n",
    "print(mean_loss_list)\n",
    "print(iterations_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xdtgideekfHP"
   },
   "outputs": [],
   "source": [
    "cmap = cm.get_cmap('summer')\n",
    "plt.title(\"Policy Iteration Performance based on K\")\n",
    "plt.ylim(0, 1)\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Mean Loss\")\n",
    "for i in range(len(K_list)):\n",
    "    plt.plot(list(range(iterations_list[i])),\n",
    "             mean_loss_list[i], color=cmap(i/len(K_list)),\n",
    "             label='K='+str(K_list[i]))\n",
    "    if i == 1:\n",
    "        plt.text(iterations_list[i]-1.5,mean_loss_list[i][1]+0.04,str(int(elapsed_time_list[i]*1000))+\" ms\",color=cmap(i/len(K_list)))\n",
    "    else:\n",
    "        plt.text(iterations_list[i]-1.5,mean_loss_list[i][1]+0.01,str(int(elapsed_time_list[i]*1000))+\" ms\",color=cmap(i/len(K_list)))\n",
    "plt.legend()\n",
    "plt.show\n",
    "#plt.savefig(\"parameter_K_iteration.png\", dpi=300)\n",
    "print(elapsed_time_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8637GSei6nIP"
   },
   "source": [
    "## Experimenting with $u_{min}$ values\n",
    "$u_{min}$ = [0.2, 0.4, 0.6, 0.8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vi4FcyilkfHQ",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mean_loss_list = []\n",
    "iterations_list = []\n",
    "elapsed_time_list = []\n",
    "u_min_list = [0.2, 0.4, 0.6, 0.8]\n",
    "for u_min_local in u_min_list:\n",
    "    opt_policy, mean_loss, iteration, elapsed_time = policy_iteration(K, N, u, u_min_local, a, q, reward, 0)\n",
    "    mean_loss_list.append(mean_loss)\n",
    "    iterations_list.append(iteration)\n",
    "    elapsed_time_list.append(elapsed_time)\n",
    "print(mean_loss_list)\n",
    "print(iterations_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zYbsdUYykfHQ"
   },
   "outputs": [],
   "source": [
    "cmap = cm.get_cmap('summer')\n",
    "plt.title(\"Policy Iteration Performance based on $u_{min}$\")\n",
    "plt.ylim(0, 1)\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Mean Loss\")\n",
    "for i in range(len(u_min_list)):\n",
    "    plt.plot(list(range(iterations_list[i])),\n",
    "             mean_loss_list[i], color=cmap(i/len(u_min_list)),\n",
    "             label='$u_{min}$='+str(u_min_list[i]))\n",
    "    if i == 1:\n",
    "        plt.text(iterations_list[i]-1.5,mean_loss_list[i][1]-0.04,str(int(elapsed_time_list[i]*1000))+\" ms\",color=cmap(i/len(u_min_list)))\n",
    "    else:\n",
    "        plt.text(iterations_list[i]-1.5,mean_loss_list[i][1]+0.01,str(int(elapsed_time_list[i]*1000))+\" ms\",color=cmap(i/len(u_min_list)))\n",
    "plt.legend()\n",
    "plt.show\n",
    "#plt.savefig(\"parameter_u_min_iteration.png\", dpi=300)\n",
    "print(elapsed_time_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g8IYNCVE67wC"
   },
   "source": [
    "## Running an example with the default values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "M-NnUvgNkfHQ",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "opt_policy, mean_loss, iteration, elapsed_time = policy_iteration(K, N, u, u_min, a, q, reward, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "epT3p1Lb_XN3"
   },
   "source": [
    "2. Q-learning, assuming parameters $α$ and $u_{min}$ are not known (but the $u_{ij}$\n",
    "relevance values, and $q$ are still known)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d45zrehhgRAg"
   },
   "source": [
    "### Q-Learning Algorithm\n",
    "\n",
    "- Initially $Q_0(s,a)=0$ $\\forall s,a$\n",
    "- Choose a random initial state\n",
    "- while True:\n",
    "  - sample action $a$\n",
    "  - get next state $s'$\n",
    "  - if $s'$ is terminal:\n",
    "    - $target=\\mathcal{R}_s$\n",
    "    - sample new random initial state $s'$\n",
    "  - else:\n",
    "    - $target = \\mathcal{R}_s+(1-q) \\max _{a'}Q(s',a')$\n",
    "  - $Q_{i+1}(s,a)=(1-\\mathfrak{a})Q_i(s,a)+\\mathfrak{a}\\cdot target$\n",
    "  - $s=s'$\n",
    "\n",
    "####How to sample action $a$?\n",
    "$\\epsilon$-Greedy\n",
    "- with probability $\\epsilon(s,t)$\n",
    "  - randomly choose $a$\n",
    "- with probability $1-\\epsilon(s,t)$\n",
    "  - $a=argmax_a Q_i(s,a)$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "CYsVRAEmbwz2"
   },
   "outputs": [],
   "source": [
    "def q_learning(K, N, q, u, u_min, a, reward, opt_policy):\n",
    "    # Initialize Q which is a list of dictionaries\n",
    "    Q = [{} for _ in range(K)]\n",
    "    opt_policy = create_matrix(opt_policy)  # Get the policy from policy iteration as a matrix\n",
    "    policy = np.zeros((K,N))\n",
    "    old_policy = np.zeros((K,N))\n",
    "    num_possible_actions = int(math.factorial(K-1)/(math.factorial(N)*math.factorial(K-N-1)))   # Number of possible recommendations for each state\n",
    "    for s in range(K):                          # For all states create the dictionary containing all recommendation tuples\n",
    "        all_recoms = possible_recom(K, N, s)\n",
    "        for recom in all_recoms:                # Initialize all values to 0\n",
    "            Q[s][tuple(recom)]=0\n",
    "\n",
    "    s = random.randint(0, K-1)  # Initial state\n",
    "    epsilon = 1                 # Initial epsilon is 1\n",
    "    round = 0                   # Round is the total number of user video choises\n",
    "    epsilon_hist = []           # A history of all epsilon values\n",
    "    round_hist = []             # The history of rounds\n",
    "    alpha_hist = []             # The history of learning rate\n",
    "    episode = 0                 # Episodes initially 0\n",
    "    start_time = time.time()    # Start counting time\n",
    "    alpha = 0.01\n",
    "\n",
    "    while True:\n",
    "        round += 1\n",
    "        alpha_hist.append(alpha)\n",
    "        epsilon_hist.append(epsilon)\n",
    "        round_hist.append(round)\n",
    "        # With probability epsilon get random recommendation\n",
    "        if random.uniform(0, 1) < epsilon:\n",
    "            recom = random_recommendation(K, N, s)\n",
    "        # With probability 1-epsilon get the recommendation with the largest Q value\n",
    "        else:\n",
    "            recom = list(max(Q[s],key=Q[s].get))\n",
    "        # User chooses the new state\n",
    "        new_s = user_chooses(K, N, q, u, u_min, a, recom, s)\n",
    "        # If the new state is terminal\n",
    "        if new_s == -1:\n",
    "            # Compute the new policy\n",
    "            for i in range(K):\n",
    "                recom_tuple = (list(max(Q[i],key=Q[i].get)))\n",
    "                for j in range(N):\n",
    "                    policy[i][j] = recom_tuple[j]\n",
    "            # Every 100 episodes\n",
    "            if episode % 100 == 0:\n",
    "                if episode == 0: print(str(policy.tolist()), end='')\n",
    "                #else:  print(\"\\r\" + str(policy.tolist()), end='')\n",
    "                # If the old policy from 100 episodes before stayed the same break\n",
    "                if np.array_equal(policy, old_policy):\n",
    "                    print(\"\\r\" + str(policy.tolist()), end='')\n",
    "                    break\n",
    "                # Assign new policy to the old\n",
    "                old_policy = np.array(policy)\n",
    "            # If the policy is equal to the optimal break\n",
    "            if np.array_equal(policy, opt_policy):\n",
    "                print(\"\\r\" + str(policy.tolist()), end='')\n",
    "                break\n",
    "            # Get the new initial state\n",
    "            new_s = random.randint(0, K-1)\n",
    "            episode += 1\n",
    "            # Calculate the new epsilon\n",
    "            epsilon = 0.01 + (1 - 0.01) * math.exp(-0.00005 * episode)\n",
    "            # The new state becomes the current state\n",
    "            s = new_s\n",
    "            continue\n",
    "        # Calculate the Q function's value\n",
    "        Q[s][tuple(recom)] += alpha * ( reward[new_s] + (1-q) * max(Q[new_s].values()) - Q[s][tuple(recom)])\n",
    "        s = new_s\n",
    "\n",
    "    # Calculate the time elapsed\n",
    "    end_time = time.time()\n",
    "    time_elapsed = end_time - start_time\n",
    "\n",
    "    policy = create_tuple_list(policy)\n",
    "    return policy, episode, time_elapsed\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hAWLF6UNdzHi"
   },
   "source": [
    "## Running a toy example\n",
    "$K$ = 10, $N$ = 2, $u_{min}$ = 0.5, $a$ = 1, $q$=0.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Mu7p6wiMkfHS",
    "outputId": "7c2dedd6-61fc-447a-c743-1dfe3df1f69a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State: 0 -> [1, 4, 6, 7, 8]\n",
      "State: 1 -> [0, 2, 3, 5, 6]\n",
      "State: 2 -> [1, 4, 6]\n",
      "State: 3 -> [1, 5, 8]\n",
      "State: 4 -> [0, 2, 5, 8]\n",
      "State: 5 -> [1, 3, 4, 7]\n",
      "State: 6 -> [0, 1, 2, 9]\n",
      "State: 7 -> [0, 5, 9]\n",
      "State: 8 -> [0, 3, 4, 9]\n",
      "State: 9 -> [6, 7, 8]\n",
      "[[1.0, 6.0], [0.0, 6.0], [1.0, 6.0], [1.0, 8.0], [0.0, 2.0], [1.0, 4.0], [0.0, 1.0], [0.0, 5.0], [0.0, 4.0], [2.0, 8.0]]\n",
      "Episodes needed to get to optimal policy: 30900\n",
      "[[1, 6], [0, 6], [1, 6], [1, 5], [0, 2], [1, 4], [0, 1], [0, 5], [0, 4], [0, 1]]\n",
      "[[1, 6], [0, 6], [1, 6], [1, 8], [0, 2], [1, 4], [0, 1], [0, 5], [0, 4], [2, 8]]\n"
     ]
    }
   ],
   "source": [
    "toy_K = 10\n",
    "toy_N = 2\n",
    "toy_u = [[0.0, 0.9, 0.1, 0.1, 0.9, 0.1, 0.9, 0.9, 0.9, 0.1],\n",
    "         [0.9, 0.0, 0.9, 0.9, 0.1, 0.9, 0.9, 0.1, 0.1, 0.1],\n",
    "         [0.1, 0.9, 0.0, 0.1, 0.9, 0.1, 0.9, 0.1, 0.1, 0.1],\n",
    "         [0.1, 0.9, 0.1, 0.0, 0.1, 0.9, 0.1, 0.1, 0.9, 0.1],\n",
    "         [0.9, 0.1, 0.9, 0.1, 0.0, 0.9, 0.1, 0.1, 0.9, 0.1],\n",
    "         [0.1, 0.9, 0.1, 0.9, 0.9, 0.0, 0.1, 0.9, 0.1, 0.1],\n",
    "         [0.9, 0.9, 0.9, 0.1, 0.1, 0.1, 0.0, 0.1, 0.1, 0.9],\n",
    "         [0.9, 0.1, 0.1, 0.1, 0.1, 0.9, 0.1, 0.0, 0.1, 0.9],\n",
    "         [0.9, 0.1, 0.1, 0.9, 0.9, 0.1, 0.1, 0.1, 0.0, 0.9],\n",
    "         [0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.9, 0.9, 0.9, 0.0]]\n",
    "relations = find_values_above_min(toy_u, u_min)\n",
    "for i in range(toy_K):\n",
    "    print(\"State: \"+str(i)+\" -> \"+str(relations[i]))\n",
    "toy_u_min = 0.5\n",
    "toy_a = 1\n",
    "toy_q = 0.05\n",
    "toy_reward = [0, 0, -1, -1, -1, -1, -1, -1, -1, -1]\n",
    "opt_toy_policy_q, episodes_q, time_toy = q_learning(toy_K, toy_N, toy_q, toy_u, toy_u_min, toy_a, toy_reward, opt_toy_policy)\n",
    "print()\n",
    "print(\"Episodes needed to get to optimal policy: \"+str(episodes_q))\n",
    "print(opt_toy_policy)\n",
    "print(opt_toy_policy_q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JTHDIXCYWzdF",
    "outputId": "8b71d66a-10c2-4cf9-84ad-8cecd7e638b5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[91mPolicy Iteratation\u001b[0m K = 10\n",
      "> Evaluating policy...\n",
      "> Improving policy...\n",
      "> Evaluating policy...\n",
      "> Improving policy...\n",
      "> Evaluating policy...\n",
      "> Improving policy...\n",
      "\u001b[91mSAME POLICY!\u001b[0m\n",
      "Iterations needed: 3\n",
      "Optimal Policy: \n",
      "[[2, 6], [6, 7], [6, 7], [6, 7], [1, 2], [3, 6], [1, 2], [1, 2], [0, 1], [1, 6]]\n",
      "> Evaluating policy...\n",
      "Optimal Value Function 3\n",
      "[-10.5        -10.25641026 -10.25641026 -10.25641026 -10.74358974\n",
      " -10.5         -9.74358974  -9.74358974 -10.76388889 -10.5       ]\n",
      "Mean Losses for each iteration: \n",
      "[0, 0, 0]\n",
      "\u001b[91mQ-Learning\u001b[0m K = 10\n",
      "[[2.0, 6.0], [6.0, 7.0], [6.0, 7.0], [6.0, 7.0], [1.0, 2.0], [3.0, 6.0], [1.0, 2.0], [1.0, 2.0], [0.0, 1.0], [1.0, 6.0]]\n",
      "> Running Monte Carlo sessions...\n",
      "> Running Monte Carlo sessions...\n",
      "[0.5013909857828405]\n",
      "[0.5013727896460233]\n",
      "\u001b[91mPolicy Iteratation\u001b[0m K = 13\n",
      "> Evaluating policy...\n",
      "> Improving policy...\n",
      "> Evaluating policy...\n",
      "> Improving policy...\n",
      "> Evaluating policy...\n",
      "> Improving policy...\n",
      "> Evaluating policy...\n",
      "> Improving policy...\n",
      "\u001b[91mSAME POLICY!\u001b[0m\n",
      "Iterations needed: 4\n",
      "Optimal Policy: \n",
      "[[4, 8], [2, 6], [1, 4], [1, 4], [2, 6], [0, 1], [1, 4], [1, 6], [3, 4], [0, 1], [4, 8], [1, 6], [1, 2]]\n",
      "> Evaluating policy...\n",
      "Optimal Value Function 4\n",
      "[-10.61570513  -9.74358974 -10.25641026 -10.25641026  -9.74358974\n",
      " -10.8428443  -10.25641026 -10.5        -10.5        -10.8428443\n",
      " -10.61570513 -10.5        -10.5       ]\n",
      "Mean Losses for each iteration: \n",
      "[0, 0, 0, 0]\n",
      "\u001b[91mQ-Learning\u001b[0m K = 13\n",
      "[[4.0, 7.0], [2.0, 3.0], [1.0, 4.0], [1.0, 4.0], [2.0, 3.0], [7.0, 10.0], [1.0, 4.0], [1.0, 2.0], [3.0, 4.0], [6.0, 7.0], [4.0, 12.0], [1.0, 6.0], [1.0, 2.0]]\n",
      "> Running Monte Carlo sessions...\n",
      "> Running Monte Carlo sessions...\n",
      "[0.5013909857828405, 0.5028025111510777]\n",
      "[0.5013727896460233, 0.5028115189748056]\n",
      "\u001b[91mPolicy Iteratation\u001b[0m K = 16\n",
      "> Evaluating policy...\n",
      "> Improving policy...\n",
      "> Evaluating policy...\n",
      "> Improving policy...\n",
      "> Evaluating policy...\n",
      "> Improving policy...\n",
      "\u001b[91mSAME POLICY!\u001b[0m\n",
      "Iterations needed: 3\n",
      "Optimal Policy: \n",
      "[[5, 7], [0, 6], [0, 5], [5, 14], [6, 7], [0, 7], [2, 9], [0, 5], [6, 7], [0, 5], [5, 6], [2, 9], [0, 6], [5, 6], [0, 5], [3, 5]]\n",
      "> Evaluating policy...\n",
      "Optimal Value Function 3\n",
      "[-6.44067797 -7.27161017 -7.11864407 -7.44067797 -7.59364407 -6.44067797\n",
      " -6.76271187 -7.11864407 -7.59364407 -7.11864407 -7.27161017 -7.76271187\n",
      " -7.27161017 -7.27161017 -7.11864407 -7.59364407]\n",
      "Mean Losses for each iteration: \n",
      "[0, 0, 0]\n",
      "\u001b[91mQ-Learning\u001b[0m K = 16\n",
      "[[5.0, 9.0], [0.0, 6.0], [0.0, 5.0], [2.0, 5.0], [7.0, 9.0], [0.0, 14.0], [2.0, 9.0], [0.0, 5.0], [6.0, 7.0], [0.0, 5.0], [5.0, 6.0], [4.0, 10.0], [0.0, 6.0], [5.0, 6.0], [0.0, 5.0], [3.0, 5.0]]\n",
      "> Running Monte Carlo sessions...\n",
      "> Running Monte Carlo sessions...\n",
      "[0.5013909857828405, 0.5028025111510777, 0.33607896384946134]\n",
      "[0.5013727896460233, 0.5028115189748056, 0.3367136685382241]\n",
      "\u001b[91mPolicy Iteratation\u001b[0m K = 19\n",
      "> Evaluating policy...\n",
      "> Improving policy...\n",
      "> Evaluating policy...\n",
      "> Improving policy...\n",
      "> Evaluating policy...\n",
      "> Improving policy...\n",
      "\u001b[91mSAME POLICY!\u001b[0m\n",
      "Iterations needed: 3\n",
      "Optimal Policy: \n",
      "[[6, 7], [7, 17], [6, 7], [6, 17], [6, 11], [6, 17], [16, 17], [6, 16], [10, 16], [12, 16], [6, 11], [7, 17], [6, 16], [6, 16], [4, 17], [11, 17], [6, 17], [6, 16], [6, 17]]\n",
      "> Evaluating policy...\n",
      "Optimal Value Function 3\n",
      "[-7.44067797 -7.76271186 -7.44067797 -7.44067797 -7.27161017 -7.44067797\n",
      " -6.44067797 -7.11864407 -7.51333686 -7.44067797 -7.27161017 -6.76271186\n",
      " -7.11864407 -7.11864407 -7.83537076 -7.59364407 -6.44067797 -7.11864407\n",
      " -7.44067797]\n",
      "Mean Losses for each iteration: \n",
      "[0, 0, 0]\n",
      "\u001b[91mQ-Learning\u001b[0m K = 19\n",
      "[[6.0, 7.0], [0.0, 10.0], [6.0, 7.0], [6.0, 17.0], [6.0, 11.0], [6.0, 7.0], [13.0, 16.0], [6.0, 16.0], [4.0, 16.0], [12.0, 16.0], [6.0, 11.0], [7.0, 13.0], [6.0, 16.0], [6.0, 16.0], [3.0, 5.0], [7.0, 11.0], [6.0, 7.0], [6.0, 16.0], [6.0, 7.0]]\n",
      "> Running Monte Carlo sessions...\n",
      "> Running Monte Carlo sessions...\n",
      "[0.5013909857828405, 0.5028025111510777, 0.33607896384946134, 0.3379819550156861]\n",
      "[0.5013727896460233, 0.5028115189748056, 0.3367136685382241, 0.33865848308453994]\n",
      "\u001b[91mPolicy Iteratation\u001b[0m K = 22\n",
      "> Evaluating policy...\n",
      "> Improving policy...\n",
      "> Evaluating policy...\n",
      "> Improving policy...\n",
      "> Evaluating policy...\n",
      "> Improving policy...\n",
      "> Evaluating policy...\n",
      "> Improving policy...\n",
      "> Evaluating policy...\n",
      "> Improving policy...\n",
      "\u001b[91mSAME POLICY!\u001b[0m\n",
      "Iterations needed: 5\n",
      "Optimal Policy: \n",
      "[[11, 16], [7, 14], [11, 13], [10, 16], [11, 13], [2, 17], [0, 2], [2, 14], [13, 17], [2, 14], [2, 14], [2, 13], [11, 16], [2, 11], [13, 17], [2, 13], [7, 17], [11, 14], [14, 16], [13, 14], [11, 16], [13, 14]]\n",
      "> Evaluating policy...\n",
      "Optimal Value Function 5\n",
      "[-7.3607478  -7.73204197 -6.44067797 -7.77661079 -7.44067797 -7.53450705\n",
      " -7.95624542 -7.31617899 -7.85654095 -7.31617899 -7.31617899 -6.44067797\n",
      " -7.3607478  -7.11864407 -6.85654095 -7.44067797 -6.95037004 -7.31617899\n",
      " -7.55828272 -7.63821288 -7.3607478  -7.63821288]\n",
      "Mean Losses for each iteration: \n",
      "[0, 0, 0, 0, 0]\n",
      "\u001b[91mQ-Learning\u001b[0m K = 22\n",
      "[[1.0, 2.0], [0.0, 2.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 2.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]"
     ]
    }
   ],
   "source": [
    "mean_loss_list_pi = []\n",
    "mean_loss_list_q = []\n",
    "episodes_list_q= []\n",
    "a = 1.0\n",
    "for K_local in range(10,35,3):\n",
    "    sessions_num = 100000\n",
    "    u_local = create_symmetric_matrix(K_local)\n",
    "    C_local = int(np.floor(0.2*K_local))\n",
    "    reward_local = random_cached_items(K_local, C_local)\n",
    "    print(\"\\033[91mPolicy Iteratation\\033[0m\"+\" K = \"+str(K_local))\n",
    "    opt_policy_local, mean_loss_local, iteration_local, elapsed_time_local = policy_iteration(K_local, N, u_local, u_min, a, q, reward_local, 0)\n",
    "    print(\"\\033[91mQ-Learning\\033[0m\"+\" K = \"+str(K_local))\n",
    "    q_policy_local, episodes_local, time_elapsed_local = q_learning(K_local, N, q, u_local, u_min, a, reward_local, opt_policy_local)\n",
    "    print()\n",
    "    mean_loss_q, total_loss_q = monte_carlo_sessions(sessions_num, q_policy_local, reward_local, K_local, N, u_min, a, q, u_local)\n",
    "    mean_loss_pi, total_loss_pi = monte_carlo_sessions(sessions_num, opt_policy_local, reward_local, K_local, N, u_min, a, q, u_local)\n",
    "    mean_loss_list_pi.append(mean_loss_pi)\n",
    "    mean_loss_list_q.append(mean_loss_q)\n",
    "    episodes_list_q.append(episodes_local)\n",
    "    print(mean_loss_list_pi)\n",
    "    print(mean_loss_list_q)\n",
    "\n",
    "    cmap = cm.get_cmap('summer')\n",
    "\n",
    "    plt.figure()\n",
    "    plt.title(\"Policy Iteration vs Q-Learning Performance\")\n",
    "    plt.ylim(0, 1)\n",
    "    plt.xlabel(\"K\")\n",
    "    plt.ylabel(\"Mean Loss\")\n",
    "    plt.plot(range(10,K_local+1,3), mean_loss_list_pi, color=cmap(0), label='Policy Iteration')\n",
    "    plt.plot(range(10,K_local+1,3), mean_loss_list_q, color=cmap(0.9), label='Q-Learning')\n",
    "    plt.legend()\n",
    "    #plt.savefig(\"pi_vs_q_performance_a_1_new.png\", dpi=300)\n",
    "\n",
    "    plt.figure()\n",
    "    plt.title(\"Q-Learning Episodes Needed\")\n",
    "    plt.xlabel(\"K\")\n",
    "    plt.ylabel(\"Number of Episodes\")\n",
    "    plt.plot(range(10,K_local+1,3), episodes_list_q, color=cmap(0.9), label='Q-Learning')\n",
    "    plt.legend()\n",
    "    #plt.savefig(\"q_episodes_a_1_new.png\", dpi=300)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GiDaxyyPkfHS"
   },
   "outputs": [],
   "source": [
    "mean_loss_list_pi = []\n",
    "mean_loss_list_q = []\n",
    "episodes_list_q_8 = []\n",
    "a = 0.8\n",
    "for K_local in range(10,35,3):\n",
    "    sessions_num = 100000\n",
    "    u_local = create_symmetric_matrix(K_local)\n",
    "    C_local = int(np.floor(0.2*K_local))\n",
    "    reward_local = random_cached_items(K_local, C_local)\n",
    "    print(\"\\033[91mPolicy Iteratation\\033[0m\"+\" K = \"+str(K_local))\n",
    "    opt_policy_local, mean_loss_local, iteration_local, elapsed_time_local = policy_iteration(K_local, N, u_local, u_min, a, q, reward_local, 0)\n",
    "    print(\"\\033[91mQ-Learning\\033[0m\"+\" K = \"+str(K_local))\n",
    "    q_policy_local, episodes_local, time_elapsed_local = q_learning(K_local, N, q, u_local, u_min, a, reward_local, opt_policy_local)\n",
    "    print()\n",
    "    mean_loss_q, total_loss_q = monte_carlo_sessions(sessions_num, q_policy_local, reward_local, K_local, N, u_min, a, q, u_local)\n",
    "    mean_loss_pi, total_loss_pi = monte_carlo_sessions(sessions_num, opt_policy_local, reward_local, K_local, N, u_min, a, q, u_local)\n",
    "    mean_loss_list_pi.append(mean_loss_pi)\n",
    "    mean_loss_list_q.append(mean_loss_q)\n",
    "    episodes_list_q_8.append(episodes_local)\n",
    "    print(mean_loss_list_pi)\n",
    "    print(mean_loss_list_q)\n",
    "\n",
    "    cmap = cm.get_cmap('summer')\n",
    "\n",
    "    plt.figure()\n",
    "    plt.title(\"Policy Iteration vs Q-Learning Performance\")\n",
    "    plt.ylim(0, 1)\n",
    "    plt.xlabel(\"K\")\n",
    "    plt.ylabel(\"Mean Loss\")\n",
    "    plt.plot(range(10,K_local+1,3), mean_loss_list_pi, color=cmap(0), label='Policy Iteration')\n",
    "    plt.plot(range(10,K_local+1,3), mean_loss_list_q, color=cmap(0.9), label='Q-Learning')\n",
    "    plt.legend()\n",
    "    #plt.savefig(\"pi_vs_q_performance_a_0_8.png\", dpi=300)\n",
    "\n",
    "    plt.figure()\n",
    "    plt.title(\"Q-Learning Episodes Needed\")\n",
    "    plt.xlabel(\"K\")\n",
    "    plt.ylabel(\"Number of Episodes\")\n",
    "    plt.plot(range(10,K_local+1,3), episodes_list_q_8, color=cmap(0.9), label='Q-Learning')\n",
    "    plt.legend()\n",
    "    #plt.savefig(\"q_episodes_a_0_8.png\", dpi=300)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "E6IHUZvvWzdG"
   },
   "outputs": [],
   "source": [
    "mean_loss_list_pi = []\n",
    "mean_loss_list_q = []\n",
    "episodes_list_q_6 = []\n",
    "a = 0.6\n",
    "for K_local in range(10,35,3):\n",
    "    sessions_num = 100000\n",
    "    u_local = create_symmetric_matrix(K_local)\n",
    "    C_local = int(np.floor(0.2*K_local))\n",
    "    reward_local = random_cached_items(K_local, C_local)\n",
    "    print(\"\\033[91mPolicy Iteratation\\033[0m\"+\" K = \"+str(K_local))\n",
    "    opt_policy_local, mean_loss_local, iteration_local, elapsed_time_local = policy_iteration(K_local, N, u_local, u_min, a, q, reward_local, 0)\n",
    "    print(\"\\033[91mQ-Learning\\033[0m\"+\" K = \"+str(K_local))\n",
    "    q_policy_local, episodes_local, time_elapsed_local = q_learning(K_local, N, q, u_local, u_min, a, reward_local, opt_policy_local)\n",
    "    print()\n",
    "    mean_loss_q, total_loss_q = monte_carlo_sessions(sessions_num, q_policy_local, reward_local, K_local, N, u_min, a, q, u_local)\n",
    "    mean_loss_pi, total_loss_pi = monte_carlo_sessions(sessions_num, opt_policy_local, reward_local, K_local, N, u_min, a, q, u_local)\n",
    "    mean_loss_list_pi.append(mean_loss_pi)\n",
    "    mean_loss_list_q.append(mean_loss_q)\n",
    "    episodes_list_q_6.append(episodes_local)\n",
    "    print(mean_loss_list_pi)\n",
    "    print(mean_loss_list_q)\n",
    "\n",
    "    cmap = cm.get_cmap('summer')\n",
    "\n",
    "    plt.figure()\n",
    "    plt.title(\"Policy Iteration vs Q-Learning Performance\")\n",
    "    plt.ylim(0, 1)\n",
    "    plt.xlabel(\"K\")\n",
    "    plt.ylabel(\"Mean Loss\")\n",
    "    plt.plot(range(10,K_local+1,3), mean_loss_list_pi, color=cmap(0), label='Policy Iteration')\n",
    "    plt.plot(range(10,K_local+1,3), mean_loss_list_q, color=cmap(0.9), label='Q-Learning')\n",
    "    plt.legend()\n",
    "    #plt.savefig(\"pi_vs_q_performance_a_0_6.png\", dpi=300)\n",
    "\n",
    "    plt.figure()\n",
    "    plt.title(\"Q-Learning Episodes Needed\")\n",
    "    plt.xlabel(\"K\")\n",
    "    plt.ylabel(\"Number of Episodes\")\n",
    "    plt.plot(range(10,K_local+1,3), episodes_list_q_6, color=cmap(0.9), label='Q-Learning')\n",
    "    plt.legend()\n",
    "    #plt.savefig(\"q_episodes_a_0_6.png\", dpi=300)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2EG-baCqWzdG"
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.title(\"Q-Learning Episodes Needed over a\")\n",
    "plt.xlabel(\"K\")\n",
    "plt.ylabel(\"Number of Episodes\")\n",
    "plt.plot(range(10,K_local+1,3), episodes_list_q_6, color=cmap(0.9), label='a=0.6')\n",
    "plt.plot(range(10,K_local+1,3), episodes_list_q_8, color=cmap(0.5), label='a=0.8')\n",
    "plt.plot(range(10,K_local+1,3), episodes_list_q, color=cmap(0.1), label='a=1.0')\n",
    "plt.legend()\n",
    "#plt.savefig(\"q_episodes_a.png\", dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jBAVM4Y6ie7M"
   },
   "outputs": [],
   "source": [
    "sessions_num = 100000\n",
    "random_policy = [random_recommendation(K, N, i) for i in range(K)]\n",
    "mean_loss_i, total_loss_i = monte_carlo_sessions(sessions_num, opt_policy, reward, K, N, u_min, a, q, u)\n",
    "mean_loss_q, total_loss_q = monte_carlo_sessions(sessions_num, q_policy, reward, K, N, u_min, a, q, u)\n",
    "mean_loss_r, total_loss_r = monte_carlo_sessions(sessions_num, random_policy, reward, K, N, u_min, a, q, u)\n",
    "print(mean_loss_i)\n",
    "print(mean_loss_q)\n",
    "print(mean_loss_r)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XWG0SBgEAEFL"
   },
   "source": [
    "###Experiments to try and report:\n",
    "- Demonstrate that Policy Iteration indeed solves the problem optimally\n",
    "(you could try a hand-picked, toy scenario for this, not necessarily randomly chosen).\n",
    "- Show the average cost achieved for different parameters ($K$, $u_{min}$ , $\\alpha$, $q$).\n",
    "- Prove that your Q-learning algorithm correctly learns the optimal policy\n",
    "as well (in the partially model-free environment above). Demonstrate the\n",
    "convergence speed to the optimal (for different K values).\n",
    "- Try to ”break” Q-learning. At what values of K does your PC or colab\n",
    "code start going ”too slow”?"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
